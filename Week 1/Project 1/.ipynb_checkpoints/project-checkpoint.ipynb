{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#libraries\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import matplotlib as mpl\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.pyplot as pl\n",
    "import pandas as pd\n",
    "import time\n",
    "import requests\n",
    "import urllib.request\n",
    "from bs4 import BeautifulSoup\n",
    "pd.set_option('display.width', 500)\n",
    "pd.set_option('display.max_columns', 100)\n",
    "pd.set_option('display.notebook_repr_html', True)\n",
    "import seaborn as sns\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.set_context(\"poster\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "FeatureNotFound",
     "evalue": "Couldn't find a tree builder with the features you requested: lxml. Do you need to install a parser library?",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFeatureNotFound\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-601e697161cb>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0murllib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0murlopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'https://en.wikipedia.org/wiki/Billboard_Year-End_Hot_100_singles_of_1970'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mresponse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m    \u001b[0mwiki_chart\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mresponse\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0msoup\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBeautifulSoup\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwiki_chart\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"lxml\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[0mtables\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msoup\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'table'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m'class'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;34m'wikitable sortable'\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python\\lib\\site-packages\\bs4\\__init__.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, markup, features, builder, parse_only, from_encoding, exclude_encodings, **kwargs)\u001b[0m\n\u001b[0;32m    206\u001b[0m                     \u001b[1;34m\"Couldn't find a tree builder with the features you \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    207\u001b[0m                     \u001b[1;34m\"requested: %s. Do you need to install a parser library?\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 208\u001b[1;33m                     % \",\".join(features))\n\u001b[0m\u001b[0;32m    209\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    210\u001b[0m         \u001b[1;31m# At this point either we have a TreeBuilder instance in\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFeatureNotFound\u001b[0m: Couldn't find a tree builder with the features you requested: lxml. Do you need to install a parser library?"
     ]
    }
   ],
   "source": [
    "#Q1. Scraping Wikipedia for Billboard Top 100.\n",
    "\n",
    "with urllib.request.urlopen('https://en.wikipedia.org/wiki/Billboard_Year-End_Hot_100_singles_of_1970') as response:\n",
    "   wiki_chart = response.read()\n",
    "soup = BeautifulSoup(wiki_chart, \"lxml\")\n",
    "tables = soup.find('table', attrs={'class':'wikitable sortable'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "song_list = []\n",
    "tr_list = tables.find_all('tr')\n",
    "for tr in tr_list:\n",
    "    td_list = tr.find_all('td')\n",
    "    if td_list == [] :\n",
    "        td_list = []\n",
    "    else : \n",
    "        ranking = td_list[0].get_text()\n",
    "        title = td_list[1].get_text()\n",
    "        band_singer = td_list[2].get_text()\n",
    "        soup_of_link = BeautifulSoup(str(td_list), \"lxml\")\n",
    "        url = td_list[2].a[\"href\"]\n",
    "        dict_entry = {'band_singer' : band_singer,\n",
    "        'ranking' : ranking,\n",
    "        'title' : title,\n",
    "        'url' : url}\n",
    "        song_list.append(dict_entry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "song_list[1:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1.2 Generalize the previous: scrape Wikipedia from 1992 to 2014\n",
    "yearstext = {}\n",
    "for year in range(1992, 2015):\n",
    "    with urllib.request.urlopen(f'https://en.wikipedia.org/wiki/Billboard_Year-End_Hot_100_singles_of_%s' % year) as response:\n",
    "        year_text = {year : response.read()}\n",
    "        yearstext.update(year_text)\n",
    "        time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yearstext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1.3 Parse and Clean data\n",
    "def parse_year(the_year, yeartext_dict):\n",
    "    year = the_year\n",
    "    yearinfo = []\n",
    "    song = []\n",
    "    songurl = []\n",
    "    band_singer = []\n",
    "    title = []\n",
    "    url = []\n",
    "    title_text = ''\n",
    "    i = 0\n",
    "    title_string = ''\n",
    "    band_singer = ''\n",
    "    soup = BeautifulSoup(yearstext[year], \"lxml\")\n",
    "    tables = soup.find('table', attrs={'class':'wikitable sortable'})\n",
    "    tr_list = tables.find_all('tr')    \n",
    "    for tr in tr_list:\n",
    "        td_list = tr.find_all('td')\n",
    "        if td_list == [] :\n",
    "            td_list = []\n",
    "        else : \n",
    "            ranking = tr.th.string\n",
    "            links = tr.td.findAll('a')\n",
    "            number_of_links = len(links)   \n",
    "            if number_of_links == 0:\n",
    "                songurl = [None]\n",
    "                title_text = [a['title']]\n",
    "                song.append(a['title'] )\n",
    "            else :\n",
    "                i = 0\n",
    "                for a in tr.td.findAll('a') : \n",
    "                    song.append(a['title'] )\n",
    "                    title_string = '\\\"' + a['title'] + '\\\"'    \n",
    "                    if i == 0 :\n",
    "                        title_text = title_string\n",
    "                        i = i + 1\n",
    "                    else :\n",
    "                        title_text = title_text + ' / ' + title_string\n",
    "                        i = i + 1    \n",
    "                    songurl.append(a['href'])\n",
    "            title = song\n",
    "            tr.td.findNext('td') \n",
    "            temp = len(tr.td.findNext('td').findAll('a'))\n",
    "            if temp == 0:\n",
    "                singer_url = [None]\n",
    "                band_singer = tr.td.findNext('td').string\n",
    "                band_singer = [band_singer]\n",
    "            elif temp == 1:\n",
    "                singer_url = tr.td.findNext('td').a['href']\n",
    "                singer_url = [singer_url]\n",
    "                band_singer = tr.td.findNext('td').a.string\n",
    "                band_singer = [band_singer]\n",
    "            else:\n",
    "                singer_url = []\n",
    "                band_singer = []\n",
    "                for a in tr.td.findNext('td').findAll('a'):\n",
    "                    webpage = a['href']\n",
    "                    singer_url.append(webpage)\n",
    "                    band_singer.append(a.string)                \n",
    "            dict_entry = {'band_singer' : band_singer,\n",
    "            'ranking' : ranking,\n",
    "            'song' : title, 'songurl': songurl, 'titletext' : title_text,\n",
    "            'url' : singer_url}\n",
    "            yearinfo.append(dict_entry)      \n",
    "            songurl = []\n",
    "            song = []\n",
    "            title_string = ''\n",
    "            title_text = ''    \n",
    "    return(yearinfo)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "years_dict = {}\n",
    "for year in range(1992, 2015):\n",
    "    years_dict.update({year : parse_year(year, yearstext)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yearinfo = years_dict "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yearinfo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parse_year(1997, yearstext)[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fd = open(\"yearinfo.json\",\"w+\")\n",
    "json.dump(yearinfo, fd)\n",
    "fd.close()\n",
    "del yearinfo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"yearinfo.json\", \"r\") as fd:\n",
    "    yearinfo = json.load(fd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1.4 Construct a year-song-singer dataframe from the yearly information\n",
    "\n",
    "rows = []\n",
    "for year in yearinfo.keys():\n",
    "    for song in yearinfo[year]:\n",
    "        song['year'] = year\n",
    "        rows.append(song) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "band_singer = []\n",
    "songurl = ''\n",
    "title_text = ''\n",
    "singer_url = []\n",
    "starting_length = len(rows)\n",
    "for dics in rows: \n",
    "    if starting_length == 0:\n",
    "        break\n",
    "    dict_add = {}\n",
    "    if len(dics['band_singer']) > 1:\n",
    "        i = 0\n",
    "        j = len(dics['band_singer'])\n",
    "        \n",
    "        for value in dics['band_singer']:\n",
    "            # new dictionary entry to append to our list\n",
    "            dict_add = {'band_singer' : dics['band_singer'][i],\n",
    "            'ranking' : dics['ranking'],\n",
    "            'song' : dics['song'], 'songurl': dics['songurl'], 'titletext' : dics['titletext'],\n",
    "            'url' : dics['url'][i], 'year' : dics['year']}\n",
    "            rows.append(dict_add)\n",
    "            i = 1 + i\n",
    "            j = j - 1\n",
    "    starting_length = starting_length - 1\n",
    "\n",
    "rows2 = []\n",
    "band_singer = []\n",
    "for dics in rows:\n",
    "    if len(dics['band_singer']) == 1 or len(dics['band_singer']) > 5:\n",
    "        rows2.append(dics)\n",
    "for row in rows2:\n",
    "    for key in row:\n",
    "        row[key] = str(row[key])\n",
    "        row[key] = row[key].strip(\"[]\")\n",
    "        row[key] = row[key].strip(\"''\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1.7 Do you notice any major differences when you change the metric?\n",
    "\n",
    "urlcache={}\n",
    "def get_page(url):\n",
    "    if (url not in urlcache) or (urlcache[url]==1) or (urlcache[url]==2):\n",
    "        time.sleep(1)\n",
    "        try:\n",
    "            r = requests.get(\"http://en.wikipedia.org%s\" % url)\n",
    "\n",
    "            if r.status_code == 200:\n",
    "                urlcache[url] = r.text\n",
    "            else:\n",
    "                urlcache[url] = 1\n",
    "        except:\n",
    "            urlcache[url] = 2\n",
    "    return urlcache[url]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flatframe = flatframe.sort_values('year')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flatframe[\"url\"].apply(get_page)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2.1 Extract information about singers and bands\n",
    "\n",
    "def singer_band_info(url, page_text):\n",
    "    bday_dict = {}\n",
    "    bday = ''\n",
    "    ya = ''\n",
    "    soup = BeautifulSoup(page_text[url], \"lxml\")\n",
    "    tables = soup.find('table', attrs={'class':'infobox vcard plainlist'})\n",
    "    if (tables == None):\n",
    "        tables = soup.find('table', attrs={'class':'infobox biography vcard'})\n",
    "    bday = tables.find('span', {'class': 'bday'})\n",
    "    if bday: \n",
    "        bday = bday.get_text()[:4]\n",
    "        bday_dict = {'url' : url, 'born' : bday, 'ya' : ya}\n",
    "    else:\n",
    "        ya = False\n",
    "        for tr in tables.find_all('tr'):\n",
    "            if hasattr(tr.th, 'span'):\n",
    "                if hasattr(tr.th.span, 'string'):\n",
    "                    if tr.th.span.string == 'Years active':                \n",
    "                        if hasattr(tr.th, 'span'):\n",
    "                            #ya = tr.td.string\n",
    "                            ya = tr.td.text   #DK add\n",
    "                            bday = 'False'\n",
    "                            bday_dict = {'url' : url, 'born' : 'False', 'ya' : ya}\n",
    "    return(bday_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = '/wiki/Iggy_Azalea'\n",
    "result = singer_band_info(url, urlcache)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2.2 Merging this information in\n",
    "list_of_addit_dicts = []\n",
    "bday_dict = {}\n",
    "for url in urlcache.keys():   \n",
    "    try:\n",
    "        bday_dict = singer_band_info(url, urlcache)\n",
    "        list_of_addit_dicts.append(bday_dict)\n",
    "    except:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_addit_dicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "additional_df = pd.DataFrame(list_of_addit_dicts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "largedf = pd.merge(flatframe, additional_df, left_on='url', right_on='url', how=\"outer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2.3 What is the age at which singers achieve their top ranking?\n",
    "new_df = pd.DataFrame(np.array(largedf[['year', 'band_singer', 'ranking', 'born']]))\n",
    "new_df.columns = ['year', 'band_singer', 'ranking', 'born']\n",
    "new_df['born'] = pd.to_numeric(new_df['born'], errors='coerce').fillna(0).astype(np.int64)                     #convert to int\n",
    "new_df['year_minus_born'] = new_df['year'] - new_df['born']\n",
    "sorted_df = new_df.sort_values(['band_singer', 'ranking']);\n",
    "filtered_df = sorted_df.drop_duplicates(subset='band_singer', keep='first')\n",
    "filtered_df = filtered_df.query('born > 0')\n",
    "print(filtered_df)\n",
    "filtered_df.groupby('band_singer')['year_minus_born'].aggregate(np.sum).sort_values(ascending=False).plot.hist(bins=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
